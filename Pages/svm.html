<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Support Vector Machines (SVM) - PCOS Analysis</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>

    <!-- Header -->
    <header>
        <h1>Support Vector Machines (SVM) for PCOS Prediction</h1>
        <nav>
            <ul>
                <li><a href="introduction.html">Introduction</a></li>
                <li><a href="eda.html">PCOS Data</a></li>
                <li><a href="models.html">Prediction Models</a></li>
                <li><a href="svm.html" class="active">SVM</a></li>
                <li><a href="ensemble.html">Ensemble</a></li>
                <li><a href="conclusion.html">Conclusions</a></li>
            </ul>
        </nav>
    </header>
    <section class="content">
        <h2>üî∑ Overview: What is a Support Vector Machine (SVM)?</h2>
        <p>
            A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification tasks. 
            Its main goal is to find the optimal boundary (called a hyperplane) that separates different classes of data points with the maximum margin ‚Äî the largest distance possible from the nearest points of each class.
        </p>
        
        <p>
            In two dimensions, this boundary is a straight line; in three dimensions, it is a flat plane; and in higher dimensions, it becomes a hyperplane. 
            SVMs are effective because they focus only on the most important data points ‚Äî called <strong>support vectors</strong> ‚Äî that lie closest to the boundary, ensuring the model is robust and not easily influenced by outliers.
        </p>
    
        <p>
            When the data is perfectly linearly separable, SVM draws a straight line to divide the classes. 
            However, if the data is not linearly separable (i.e., classes overlap or curve around each other), SVMs can still find a separator by using special mathematical techniques called <strong>kernels</strong> that project the data into higher dimensions.
        </p>
    
        <p>
            Thanks to their versatility and strong theoretical foundation, SVMs are widely used for tasks such as image recognition, text classification, and medical diagnosis, especially when the number of features is large compared to the number of data points.
        </p>

        <div class="image-container-svm">
            <img src="../assests/svm.png" alt="Polynomial Kernel Separator" class="svm-image">
        </div>
        


    </section>
    
    <section class="content">
        <h2>üîµ Why SVMs are Linear Separators</h2>
        <p>
            Support Vector Machines (SVMs) are powerful algorithms that aim to find the best boundary, or hyperplane, that separates two classes of data points. 
            In their simplest form, SVMs work by drawing a straight line (in 2D) or a flat hyperplane (in higher dimensions) that divides the data into two groups with the maximum possible margin between them. 
            The reason they are called "linear separators" is because the decision boundary is a straight line ‚Äî it is based on a simple linear equation of the form <code>w¬∑x + b = 0</code>, where <code>w</code> is the weight vector and <code>b</code> is the bias.
            SVMs focus only on the critical points (called support vectors) that are closest to the boundary, ensuring the separation is as wide and robust as possible.
        </p>
    </section>
    
    <section class="content">
        <h2>üü£ The Importance of Dot Product in Kernels</h2>
        <p>
            The dot product (also called inner product) is at the core of how SVMs operate, especially when using kernels. 
            In simple linear SVMs, the dot product measures the similarity between two points by calculating the angle between them. 
            When we use kernels to create nonlinear boundaries, we are essentially replacing the standard dot product with a new function that computes similarity in a higher-dimensional space ‚Äî without ever having to compute the coordinates directly!
        </p>
        <p>
            This is known as the <strong>kernel trick</strong>. Instead of manually transforming the data into higher dimensions, the kernel trick allows SVMs to still use the dot product idea but in a more complex space. 
            This makes it possible to separate even highly tangled data in lower dimensions by projecting it into a space where a linear separator is possible.
        </p>
    </section>


    <section class="content">
        <h2>üîÑ Polynomial Kernel Mapping Example</h2>
    
        <p>
            To understand how kernels transform data, we manually mapped a 2D point (2, 3) into a 6D space using a <strong>Polynomial Kernel</strong> with constant <strong>r = 1</strong> and degree <strong>d = 2</strong>. 
            This mapping expands the input features into additional higher-order terms, enabling the Support Vector Machine (SVM) to discover more complex boundaries that are not visible in the original space.
        </p>
    

        <h2>üß© Visualizing SVM Decision Boundaries</h2>
    
        <p>
            SVMs with a <strong>Linear Kernel</strong> create a straight-line separator between classes. This approach works well when the data is linearly separable.
        </p>
        <div class="image-container-svm">
            <img src="../assests/SVM_Kernel_linear.png" alt="Linear SVM Separator" class="svm-image">
        </div>
    
        <p>
            When the data is not linearly separable, a <strong>Polynomial Kernel</strong> transforms the input space so that a curved boundary can separate the classes more effectively.
        </p>
        <div class="image-container-svm">
            <img src="../assests/SVM_Kernel_polyno.png" alt="Polynomial Kernel Separator" class="svm-image">
        </div>
    </section>
    
    <section class="content">
        <h2>üìä Access Full SVM Code on GitHub</h2>
        <p>
            You can view the full SVM implementation code, including data cleaning, splitting, model training with various kernels, and evaluation, through the link below:
        </p>
        <a href="https://github.com/Simrann020/PCOS/blob/master/Model/SVM.ipynb" target="_blank" class="github-button">View SVM Code on GitHub</a>
        
    </section>
    
    <section class="content">
        <h2>Support Vector Machine (SVM) Data Preparation & Code</h2>
    
        <h3>1Ô∏è‚É£ Core PCOS Dataset Preparation</h3>
        <p>
            For the Support Vector Machine (SVM) implementation, we worked with the <strong>Core PCOS Cleaned Dataset</strong>. 
            The process involved carefully preparing the data to ensure it was fully numeric and labeled, making it suitable for SVM modeling.
        </p>
    
        <h3>1Ô∏è‚É£ Cleaned Dataset</h3>
        <p>
            The preprocessing steps included:
        </p>
        <ul>
            <li>Stripping unnecessary whitespace from column names.</li>
            <li>Selecting important features such as <strong>Age (yrs)</strong>, <strong>BMI</strong>, <strong>hair growth</strong>, <strong>Hair loss</strong>, <strong>Pimples</strong>, and the target <strong>PCOS (Y/N)</strong>.</li>
        </ul>
        <p>
            Below is a snapshot of the before cleaning dataset:
        </p>
        <div class="image-container-Naive">
            <img src="../assests/Merged1.png" alt="Cleaned Core PCOS Dataset for SVM" class="pcos-image">
        </div>
        <p>
            Below is a snapshot of the cleaned dataset, prepared for SVM training:
        </p>
        <div class="image-container-Naive">
            <img src="../assests/SVM_Clean.png" alt="Cleaned Core PCOS Dataset for SVM" class="pcos-image">
        </div>
    
        <h3>2Ô∏è‚É£ Sample Train and Test Data</h3>
        <p>
            After cleaning, the data was split into <strong>80% training</strong> and <strong>20% testing</strong> sets using scikit-learn's <code>train_test_split</code>. 
            This ensures a disjoint split between training and testing data, critical for fair model evaluation.
        </p>
        <div class="image-container-Naive">
            <img src="../assests/SVM_Xtrain.png" alt="SVM Train Data Sample" class="pcos-image">
            <img src="../assests/SVM_Xtest.png" alt="SVM Test Data Sample" class="pcos-image">
        </div>
    
        <p>The output below confirms the shape of the training and testing datasets:</p>
        <div class="image-container-Naive">
            <img src="../assests/SVM_Shape.png" alt="SVM Training and Testing Set Shape" class="pcos-image">
        </div>
    
    
        
    </section>
    
   <section class="content">
    <h2>üîé SVM Model Training and Results</h2>

    <p>
        After preparing the labeled numeric dataset, we trained three different Support Vector Machine (SVM) classifiers using <strong>Linear</strong>, <strong>Polynomial</strong>, and <strong>RBF (Radial Basis Function)</strong> kernels. 
        For each model, we tested three different <strong>C values (0.1, 1, 10)</strong> to observe how regularization strength affects model performance. Accuracy, confusion matrices, and classification reports were evaluated.
    </p>

    <h3>1Ô∏è‚É£ Linear Kernel</h3>
    <p>
        The Linear Kernel SVM attempts to separate the classes using a straight hyperplane. 
        It is best suited for linearly separable data. Across all C values, the Linear Kernel achieved strong and stable results:
    </p>
    <ul>
        <li><strong>C=0.1:</strong> Accuracy = 75%</li>
        <li><strong>C=1:</strong> Accuracy = 75%</li>
        <li><strong>C=10:</strong> Accuracy = 75%</li>
    </ul>
    <p>
        The model consistently predicted the majority class (non-PCOS) correctly but exhibited moderate false negatives, indicating some PCOS cases were missed.
    </p>

    <h3>2Ô∏è‚É£ Polynomial Kernel</h3>
    <p>
        The Polynomial Kernel maps the features into a higher-dimensional space using polynomials (degree=3). 
        While offering more flexibility, it slightly overfitted the data compared to the Linear Kernel:
    </p>
    <ul>
        <li><strong>C=0.1:</strong> Accuracy = 75%</li>
        <li><strong>C=1:</strong> Accuracy = 73%</li>
        <li><strong>C=10:</strong> Accuracy = 74%</li>
    </ul>
    <p>
        The Polynomial Kernel introduced more false positives and false negatives at higher C values, suggesting some overfitting, especially when the cost was increased.
    </p>

    <h3>3Ô∏è‚É£ RBF Kernel</h3>
    <p>
        The RBF Kernel projects the data into an infinite-dimensional space to model complex non-linear relationships. 
        It performed competitively with Linear SVM, showing slightly variable results:
    </p>
    <ul>
        <li><strong>C=0.1:</strong> Accuracy = 72%</li>
        <li><strong>C=1:</strong> Accuracy = 75%</li>
        <li><strong>C=10:</strong> Accuracy = 72%</li>
    </ul>
    <p>
        The RBF Kernel struggled with low C values but matched Linear Kernel performance when C=1. This shows that RBF needed some flexibility (lower bias) but also risked slight overfitting at high C.
    </p>

    <h3>üìä Comparative Summary</h3>
    <table class="comparison-table">
        <thead>
            <tr>
                <th>Kernel Type</th>
                <th>C Value</th>
                <th>Accuracy (%)</th>
                <th>Observations</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Linear</td>
                <td>0.1</td>
                <td>75%</td>
                <td>Stable and high accuracy; slight bias toward non-PCOS.</td>
            </tr>
            <tr>
                <td>Linear</td>
                <td>1</td>
                <td>75%</td>
                <td>Same strong performance; balanced generalization.</td>
            </tr>
            <tr>
                <td>Linear</td>
                <td>10</td>
                <td>75%</td>
                <td>Maintained high accuracy; minimal overfitting observed.</td>
            </tr>
            <tr>
                <td>Polynomial</td>
                <td>0.1</td>
                <td>75%</td>
                <td>Good but began to slightly overfit complex patterns.</td>
            </tr>
            <tr>
                <td>Polynomial</td>
                <td>1</td>
                <td>73%</td>
                <td>Lower performance; more misclassifications.</td>
            </tr>
            <tr>
                <td>Polynomial</td>
                <td>10</td>
                <td>74%</td>
                <td>Improved but still slightly less stable than Linear SVM.</td>
            </tr>
            <tr>
                <td>RBF</td>
                <td>0.1</td>
                <td>72%</td>
                <td>Underfit at low C; missed PCOS cases.</td>
            </tr>
            <tr>
                <td>RBF</td>
                <td>1</td>
                <td>75%</td>
                <td>Best RBF performance; matched Linear SVM.</td>
            </tr>
            <tr>
                <td>RBF</td>
                <td>10</td>
                <td>72%</td>
                <td>Overfit and slight decrease in recall for PCOS detection.</td>
            </tr>
        </tbody>
    </table>

    <h3>üì∏ Best Decision Boundary Visualizations</h3>
    <p>Below are the decision boundary visualizations for the best model settings for each kernel.</p>

    <div class="image-container-svm">
        <div>
            <h4>Linear Kernel (C=1)</h4>
            <img src="../assests/SVM_Linear_Boundary.png" alt="Linear SVM Decision Boundary" class="pcos-image">
        </div>
        <div>
            <h4>Polynomial Kernel (C=0.1)</h4>
            <img src="../assests/SVM_Polynomial_Boundary.png" alt="Polynomial SVM Decision Boundary" class="pcos-image">
        </div>
        <div>
            <h4>RBF Kernel (C=1)</h4>
            <img src="../assests/SVM_RBF_Boundary.png" alt="RBF SVM Decision Boundary" class="pcos-image">
        </div>
    </div>

</section>

<section class="content">
    <h2>üîç Conclusion</h2>
    <p>
        In this PCOS prediction study using Support Vector Machines (SVMs), the <strong>Linear Kernel</strong> consistently achieved the highest and most stable accuracy (75%) across different C values.
        This indicates that the PCOS dataset is relatively structured and linearly separable without needing complex transformations.
    </p>
    <p>
        The <strong>Polynomial Kernel</strong> and <strong>RBF Kernel</strong> introduced slight overfitting and variability at higher C values, which made their performance less stable compared to the Linear Kernel.
        Overall, a simple linear separator provided the best balance between precision, recall, and generalization for our dataset.
    </p>
    <p>
        These findings suggest that straightforward models like Linear SVM can be highly effective for medical datasets like PCOS, especially when proper feature selection and preprocessing are done.
        Complex kernels should only be considered when clear non-linear patterns exist in the data.
    </p>
</section>
  


    
    <!-- Footer -->
    <footer>
        <p>¬© 2025 Simran Jadhav | PCOS Data Science Project</p>
    </footer>

</body>
</html>
